\chapter{3D-Segmentation Models in Pytorch} \label{code_implementation}
The following code is a modification of the original implementation \cite{IMvision12}, with adjustments and enhancements made to handle \gls{3d} image data effectively. In the following sections, the modified code for all modules needed to perform image segmentation on \gls{3d} images is shown.

\medskip

The code for \gls{shvit} in section \ref{sec::shvid_3d.py} is based on the original implementation from \cite{github_shvit} and has been slightly modified to function as a standalone version without requiring additional package installations. 

\medskip

Note that in this thesis also code for \gls{2d} image segmentation is used but only the code for \gls{3d} segmentation is shown in the sections below. The \gls{2d} version can be implemented by removing all \gls{3d}-specific parts and replacing them with their \gls{2d} counterparts (i.e.~{\tt Conv3D} by {\tt Conv2D}, or remove code with {\tt D=D} and the lines that use the shape of depth).

\medskip

The full source code used in this thesis is available on GitHub of \gls{fau}: \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba}


\section{Sourcecode for 3D-Segformer models} \label{sec::segformer_main}
The main models are located in the file {\tt segformer\_3d.py}. Here the individual modules for Mix-Vision Transformer, \gls{shvit} and SegformerHead are combined to form the final Segformer models. The source code for the remaining modules is organized into separate program files, with links to the corresponding GitHub repository provided below.


\subsection{{\tt segformer\_3d.py}}
The code implements the 3D segmentation models {\tt SegFormer3D} and {\tt SegFormer3D\_SHViT}. 

\medskip

\noindent Key Components:
\begin{itemize}
	\item {\tt SegFormer3D}: The main segmentation model that integrates {\tt MixVisionTransformer3D}, {\tt SegFormerHead3D}, and {\tt ResizeLayer3D}, and supports different configurations specified in {\tt MODEL\_CONFIGS}.

	\item {\tt SegFormer3D\_SHViT}: The main segmentation model integrates {\tt SHViT3D}, {\tt SegFormerHead3D}, and {\tt ResizeLayer3D}, and supports different configurations specified in {\tt MODEL\_CONFIGS} and {\tt SHVIT\_CONFIG}.
\end{itemize}

The SegFormer model implementation, along with its variants used in this thesis, is available in the project's GitHub repository: \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/models_torch/segformer_3d.py}.



\subsection{{\tt attention\_3d.py}}
The \gls{3d} Attention Layer is a neural network module implemented using PyTorch. This layer is designed to handle 3-dimensional input data and apply a self-attention mechanism, which allows the network to focus on different parts of the input for each output. Code is based on \cite{IMvision12}, it was modified for PyTorch implementation and adapted to support 3D segmentation.

\medskip

\noindent Key features of the {\tt Attention3D} Layer:
\begin{itemize}
	\item Multi-head Attention: This helps the model focus on information from different parts of the data at the same time, improving its ability to understand various features of the input.
	
	\item Spatial Reduction: Optionally, the input data can be spatially reduced, which can help in managing large input dimensions and improving computational efficiency.
	
	\item Query, Key, and Value Projections: The layer uses linear projections to create queries, keys, and values from the input data, which are essential components of the attention mechanism.
	
	\item Dropout Mechanism: Dropout is applied both to the attention weights and the final projections to prevent overfitting and to improve the model's generalization abilities.
	
	\item Layer Normalization: When spatial reduction is applied, layer normalization helps in stabilizing and speeding up the training process.
\end{itemize}

The code for the \gls{3d} Attention Layer is available in the project's GitHub repository: \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/models_torch/attention_3d.py}.


\subsection{{\tt head\_3d.py}}
The code provides implementations of various neural network modules to handle 3-dimensional data and includes a \gls{mlp} layer, a convolutional module, and a segmentation head based on the SegFormer architecture. Code is based on \cite{IMvision12}, it was modified for PyTorch implementation and adapted to support \gls{3d} segmentation.

\medskip

\noindent Key Components:
\begin{itemize}
	\item {\tt MLP3D} Layer: This module performs a simple linear projection, effectively transforming the input tensor's dimensions.
	
	\item {\tt ConvModule3D}: A \gls{3d} Convolutional module that applies convolution, batch normalization, and ReLU activation to the input tensor.
	
	\item {\tt SegFormerHead3D}: A segmentation head designed for processing \gls{3d} tensors. It uses \gls{mlp} layers to transform features and a convolutional layer to fuse them, followed by dropout and final prediction layers.
\end{itemize}

Implementation details of the modules for processing \gls{3d} data, including an \gls{mlp} layer, a convolutional block, and a SegFormer-based segmentation head, can be found in the project's GitHub repository: \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/models_torch/head_3d.py}.



\subsection{{\tt modules\_3d.py}}
The code describes a \gls{3d} Vision Transformer model with various neural network modules, including a depth-wise convolution layer, a multi-layer perceptron (\gls{mlp}) with depth-wise convolution, a transformer block with multi-head self-attention, overlapping patch embedding, and the main transformer model is available at the project's GitHub repository: \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/models_torch/modules_3d.py}. The code is based on \cite{IMvision12}, it was modified for PyTorch implementation and adapted to support \gls{3d} segmentation.

\medskip

\noindent Key Components:
\begin{itemize}
	\item {\tt DWConv3D}: A depth-wise convolution layer designed to process \gls{3d} data, applying convolution across each spatial dimension.
	
	\item {\tt Mlp3D}: A multi-layer perceptron that includes linear projections, depth-wise convolution, activation, and dropout mechanisms.
	
	\item {\tt Block3D}: A transformer block that combines multi-head self-attention with an \gls{mlp}, including layer normalization and dropout.
	
	\item {\tt OverlapPatchEmbed3D}: A patch embedding layer that creates overlapping patches from the input \gls{3d} data, transforming spatial dimensions into sequence representations.
	
	\item {\tt MixVisionTransformer3D}: The main transformer model that combines patch embedding and multiple transformer blocks to process \gls{3d} data, enabling the extraction of hierarchical features from the input.
\end{itemize}




\subsection{{\tt utils\_3d.py}} \label{sec::utils_3d.py}
The code defines two utility layers aimed at enhancing the performance and usability of \gls{3d} neural network models in PyTorch. These layers include a layer for resizing \gls{3d} input tensors and another for applying the DropPath regularization technique to improve model generalization. The code is based on \cite{IMvision12}, it was modified for PyTorch implementation and adapted to support \gls{3d} segmentation.

\medskip

\noindent Key Components:
\begin{itemize}
	\item {\tt ResizeLayer3D}: This layer resizes \gls{3d} input tensors to a target depth, height, and width using trilinear interpolation.
	
	\item {\tt DropPath3D}: This layer applies the stochastic depth (DropPath) regularization technique, which randomly drops paths during training to prevent overfitting.
\end{itemize}

The code is accessible via the GitHub repository: \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/models_torch/utils_3d.py}.



\subsection{{\tt center\_attention\_3d.py}} \label{sec::center_attention_3d.py}
In the code a local (kernel‑based) central self‑attention over a \gls{3d} volume is implemented. Instead of attending globally to every voxel, it:
\begin{itemize}
	\item Slides a cubic window ($k$ kernel) over the volume. Each query only attends to its local $k^3$ neighborhood (rather than the full volume).
	\item For each center position, computes \gls{q} from that center and \gls{k}/\gls{v} from its local neighborhood.
	\item Performs scaled dot‑product attention within each cube.
	\item Outputs an updated feature for each center voxel.
\end{itemize}
The code is very efficient because it is using {\tt unfold} and batched \gls{q}/\gls{k}\gls{v} to reduce memory compared to a full self‑attention function. The code is based on \cite{Li_2022}, it was modified for PyTorch implementation and adapted to support \gls{3d} segmentation and is available here: \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/models_torch/center_attention_3d.py}




\subsection{{\tt shvit\_3d.py}} \label{sec::shvid_3d.py}
The code represents a modified version of a PyTorch module designed for visual processing tasks, such as image classification or detection, utilizing the Vision Transformer architecture. The code is based on \cite{yun2024shvit}, it was modified and adapted to support \gls{3d} segmentation.

\medskip

\noindent Key Components:
\begin{itemize}
	\item {\tt GroupNorm} Class: Implements Group Normalization for input tensors. Normalizes the input tensor across spatial dimensions (height and width).
	
	\item {\tt Conv3d\_BN} Class: Combines a \gls{3d} Convolutional layer followed by Batch Normalization. Includes an optional activation function and the ability to fuse convolution and batch normalization layers.
	
	\item {\tt make\_divisible} Function: Ensures that a value is divisible by a specified divisor, which is useful for network architectures that require certain quantities (like the number of channels) to be divisible by a specific number.
	
	\item {\tt SqueezeExcite3D} Class: Implements the \gls{se} module, which recalibrates input features based on their inter-channel dependencies.
	
	\item {\tt PatchMerging3D} Class: Implements a patch merging mechanism using convolutional layers to reduce the spatial dimensions of the input tensor while increasing the number of channels.
	
	\item {\tt Residual3D} Class: Implements a residual connection for layers, optionally including dropout.
	
	\item {\tt FFN3D} Class: Defines a \gls{ffn} with two {\tt Conv3d\_BN} layers and a {\tt ReLU} activation function between them.
	
	\item {\tt SHSA3D} Class: Defines a \gls{shsa} mechanism for processing partial input dimensions.

	\item {\tt SHCSA3D} Class: Defines the modified \gls{shsa} mechanism with central self-attention for processing partial input dimensions.
	
	\item {\tt BasicBlock3D} Class: Represents a basic block for the \gls{shvit} architecture, combining convolutional layers, self-attention, and feed-forward networks through residuals.
	
	\item {\tt SHViT3D} Class: Incorporates the main \gls{shvit} model with hierarchical stages.
	Contains a patch embedding mechanism and multiple stages, each consisting of blocks that process the input tensor through convolutional, self-attention, and feed-forward layers.
	The forward method processes the input tensor through these stages and returns the output tensors for different stages.
\end{itemize}

This code implements a Vision Transformer model with hierarchical stages, incorporating custom features like Group Normalization, Squeeze-and-Excitation, and Residual connections: \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/models_torch/shvit_3d.py}. The goal is to enhance the Vision Transformer's performance by integrating these additional components for improved feature processing and representation.


\subsection{{\tt shoes3D\_torch\_B5\_S2\_SW\_001\_2conv3d\_224x224x224.ipynb}} \label{sec::standard'odell_nb}
The filename follows a structured naming convention that encodes key details about the notebook’s purpose, dataset, model architecture, configuration settings, and input data dimensions. The notebook implements a \gls{3d} convolutional neural network in PyTorch for classifying \gls{3d} voxel representations of shoe models, using the code described in the previous sections \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/shoes3D_torch_B5_S2_SW_001_2conv3d_224x224x224.ipynb}.

\begin{itemize}
	\item \textbf{Imports:} Load all necessary libraries, including PyTorch, NumPy, and Matplotlib.
	\item \textbf{Configuration:} Define key training parameters such as {\tt EPOCHS}, {\tt BATCH\_SIZE}, and {\tt LEARNING\_RATE}, and set paths to the training and test datasets.
	\item \textbf{Dataset:} Implement the {\tt Shoe3DDataset} class, a custom PyTorch {\tt Dataset} for loading and preprocessing \gls{3d} .npy files, extracting labels, and applying optional data augmentations.
	\item \textbf{Model:} Definition of \gls{shvit} with central self-attention
	\item \textbf{Callbacks:} Set up training utilities such as warm-up scheduling, csv-logger, early stopping, and model checkpointing.
	\item \textbf{Training:} Train the model using gradient accumulation to support small batch sizes.
	\item \textbf{Visualization:} Plot training and validation losses as well as F1-scores over epochs.
	\item \textbf{Evaluation:} Evaluate the final trained model on the test dataset and report performance metrics.
\end{itemize}


\subsection{Tools}
Several files were used for dataset preparation, visualization, and for generating the plots presented in this thesis:
\begin{itemize}
	\item {\tt 01\_visualize\_and\_extract\_2D.ipynb}: Generates \gls{2d} visualizations of the shoes (figure~\ref{Bruschi_down2_2_2}) and corresponding segmentation masks (figure~\ref{Bruschi_down2_2_2_Segmentation}) \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/01_visualize_and_extract_2D.ipynb}. The paths to the shoe scan and annotation files must be adjusted accordingly.
	\item {\tt 01\_visualize\_and\_extract\_3D.ipynb}: Creates the \gls{3d} dataset, including both shoe and segmentation files, for a predefined shape and number of layers \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/01_visualize_and_extract_3D.ipynb}. As above, the paths to the shoe scans and annotation files need to be modified.
	\item {\tt 3d-plot.py}: A standalone script to visualize a \gls{3d} rendering of a shoe object (see figure~\ref{Bruschi_down2_2_2_3d}) \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/3d-plot.py}.
	\item {\tt plots.ipynb}: A Jupyter notebook used to generate the plots shown in chapter~\ref{Results_and_Discussion} \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/experiment_kFold/plots.ipynb}.
	\item {\tt legende.ipynb}: A Jupyter notebook to generate the legend used in images, see figures \ref{result_for_shvit_with_center_attention}, \ref{comparison_segmentations_for_different_volume_sizes}, \ref{Paper_3Segments}, \ref{Paper_2Segments}, \ref{Paper_4Segments} \url{https://git5.cs.fau.de/uw61ehod/shvit_segformer_ba/-/blob/main/legende.ipynb}.
\end{itemize}
