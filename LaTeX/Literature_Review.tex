\chapter{Literature Review}

\section{Overview of Transformer-Based Models}
Transformer-based models have revolutionized the field of artificial intelligence and machine learning, particularly in the domains of\gls{nlp} and computer vision. Initially introduced by Vaswani et al. in 2017 \cite{vaswani2023attentionneed}, transformers have rapidly become the architecture of choice due to their exceptional performance and flexibility.  In 2021, a research team at Google introduced the paper \enquote{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale} in 2021, which applied the Transformer encoder architecture to the image recognition (classification) task \cite{dosovitskiy2021imageworth16x16words}.

The transformer architecture is fundamentally different from previous sequence-to-sequence models like \glspl{rnn} and \gls{lstm} networks. It relies on an attention mechanism to process input data in parallel, rather than sequentially. This parallelization enables transformers to handle long-range dependencies more effectively and efficiently.

The key components of transformer models are:
\begin{itemize}
	\item Self-Attention Mechanism: Self-attention allows the model to weigh the importance of different words in a sentence relative to each other, capturing contextual relationships.
	\item Multi-Head Attention: This involves multiple attention layers (heads) that operate in parallel, enabling the model to focus on different parts of the input simultaneously.
	\item Positional Encoding: Because transformers do not inherently understand the order of sentences or words, positional encoding is used to inject information about the relative or absolute position of tokens in the sequence.
	\item Feedforward Neural Networks: Each transformer block contains a feedforward neural network that processes the attention outputs and introduces nonlinearity.
	\item Layer Normalization: Normalization layers stabilize and accelerate training by reducing internal covariate shift in the model.
\end{itemize}

For Vision Transformer the transformer architecture is extended to image data by dividing images into patches and processing them similarly to token sequences in \gls{nlp}.


\section{Image Segmentation}
Image segmentation is a critical task in the field of computer vision, aiming to partition an image into meaningful regions or segments, often corresponding to different objects or areas of interest. Unlike image classification, which assigns a single label to an entire image, segmentation seeks to label each pixel, providing a more granular understanding of the visual content.

There are several types of image segmentation, each serving a different purpose:
\begin{itemize}
	\item Semantic Segmentation: This involves classifying each pixel in the image into a class without distinguishing between different instances of the same class. For example, all pixels belonging to 'cars' will be labeled as 'car', regardless of the number of cars present.
	\item Instance Segmentation: This not only classifies each pixel but also distinguishes between different instances of the same class. For example, each car in an image will be individually labeled.
	\item Panoptic Segmentation: This combines both semantic and instance segmentation, providing a complete scene understanding by labeling all pixels and distinguishing different instances.
\end{itemize}

Evaluating the performance of segmentation models involves several metrics, including:
\begin{itemize}
	\item \gls{iou} or Jaccard index, which is the ratio of the intersection area to the union area of the predicted ($A$) and ground truth segments ($B$): 
	\begin{equation}
		\text{IoU} = \frac{\left| A \cap B \right|}{\left| A \cup B \right|} = \frac{\left| A \cap B \right|}{\left| A \right| + \left| B \right| - \left| A \cap B \right|}
	\end{equation}
	The Jaccard Distance Loss is the defined as: 
	\begin{equation}
		\text{Jaccard-Distance-Loss} = 1 - \text{IoU}
	\end{equation} If there is no intersection between the two classes $A$ and $B$, then $\left| A \cap B \right| = 0$ and $\text{IoU}=0$ and $\text{Jaccard-Distance-Loss} = 1$. In an experiment with multiple segmentations \gls{iou}  is averaged over all single \glspl{iou}: 
	\begin{equation}
		\text{IoU} = \frac{1}{N} \sum_{i=1}^N \text{IoU}_i
	\end{equation}
	\item Dice Coefficient: Similar to \gls{iou} but gives more weight to correctly predicted areas, particularly useful for dealing with imbalanced datasets. 
	\item Pixel Accuracy: The ratio of correctly classified pixels to the total number of pixels. However, for imbalanced data this metric shouldn't be used.
\end{itemize}

Modern segmentation models, such as \gls{shvit} and Segformer, leverage transformer architectures to enhance model performance and flexibility. These models integrate self-attention mechanisms to capture global context and fine-grained details, pushing the boundaries of segmentation quality and efficiency.


\section{Segformer: Architecture, Applications, and Performance}
Segformer is a state-of-the-art image segmentation model that leverages the power of transformers to achieve high-performance on various segmentation tasks. Introduced by researchers at NVIDIA, Segformer addresses the limitations of conventional \glspl{cnn} and transformer-based models by combining their strengths to provide an efficient and scalable solution for image segmentation.

Segformer adopts a unique architecture that integrates the advantages of transformers with the computational efficiency of \glspl{cnn}. The model comprises two main components:
\begin{itemize}
	\item Hierarchical Transformer Encoder: The encoder uses hierarchical transformers to capture multi-scale contextual information from the input image. Unlike traditional vision transformers that process image patches of a fixed size, Segformer employs a hierarchical structure to progressively reduce the spatial resolution while increasing the receptive field. This allows the model to capture fine-grained details at higher resolutions and global context at lower resolutions.
	
	\item Lightweight All-MLP Decoder: The decoder in Segformer is built using multi-layer perceptrons (MLPs) instead of convolutional layers. This lightweight design reduces computational complexity and makes the model more efficient. The decoder aggregates the multi-scale features extracted by the encoder and generates dense segmentation maps.
\end{itemize}



\section{SHViT: Architecture, Applications, and Performance}

\section{Analysis of 2D and 3D Modeling Techniques}

\section{Comparative Analysis of SHViT and Segformer}

\section{Summary}
